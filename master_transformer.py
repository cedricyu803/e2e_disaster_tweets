# -*- coding: utf-8 -*-
"""
Created on Mon Oct 25 12:00:00 2021

@author: Cedric Yu
"""

# import matplotlib.pyplot as plt
# import math
# import re


"""
#####################################

# https://www.kaggle.com/c/nlp-getting-started/overview

Competition Description
Twitter has become an important communication channel in times of emergency.
The ubiquitousness of smartphones enables people to announce an emergency theyâ€™re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).

But, itâ€™s not always clear whether a personâ€™s words are actually announcing a disaster. In this competition, youâ€™re challenged to build a machine learning model that predicts which Tweets are about real disasters and which oneâ€™s arenâ€™t. 

#####################################

# Metric
Submissions are evaluated using [F1 between the predicted and expected answers].

Submission Instructions
For each ID in the test set, you must predict 1 if the tweet is describing a real disaster, and 0 otherwise. The file should contain a header and have the following format:

id,target
0,0
2,0
3,1
9,0
11,0

#####################################

# Dataset

Files

# train.csv - the training set
# test.csv - the test set
# sample_submission.csv - a sample submission file in the correct format

Columns

# id - a unique identifier for each tweet
# text - the text of the tweet
# location - the location the tweet was sent from (may be blank)
# keyword - a particular keyword from the tweet (may be blank)
# target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)


"""

# %% This file

"""
uses BertForSequenceClassification and pre-trained bert-base-uncased from hugging face transformers
https://huggingface.co/bert-base-uncased
"""

# %% Workflow

"""
# load datasets and clean text
    # get rid of html syntax using bs4.BeautifulSoup
    # keep alphabets and full stops
    # lower case

# train-validation split
# create indices for words
# uses BertForSequenceClassification and pre-trained bert-base-uncased from hugging face transformers, and re-train with our training data
# predict

"""


# %% Preamble

# Make the output look better
import tensorflow as tf
from transformers import TFBertForSequenceClassification
from transformers import BertTokenizerFast
from sklearn.preprocessing import MinMaxScaler
import os
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
# pd.set_option('display.width', 1000)
pd.set_option('display.max_colwidth', None)
# default='warn' # ignores warning about dropping columns inplace
pd.options.mode.chained_assignment = None
# import re

os.chdir(r'C:\Users\Cedric Yu\Desktop\Works\9_nlp_disaster_tweets')


# %% load engineered datasets

# !!!
X_train_text_preprocessed = np.load(
    'engineered_datasets/X_train_text_preprocessed.npy', allow_pickle=True).squeeze().tolist()
X_valid_text_preprocessed = np.load(
    'engineered_datasets/X_valid_text_preprocessed.npy', allow_pickle=True).squeeze().tolist()
X_test_text_preprocessed = np.load(
    'engineered_datasets/X_test_text_preprocessed.npy', allow_pickle=True).squeeze().tolist()

X_train_no_text_encoded2 = np.load(
    'engineered_datasets/X_train_no_text_encoded2.npy')
X_valid_no_text_encoded2 = np.load(
    'engineered_datasets/X_valid_no_text_encoded2.npy')
X_test_no_text_encoded2 = np.load(
    'engineered_datasets/X_test_no_text_encoded2.npy')

scaler = MinMaxScaler()

X_train_no_text_encoded2 = scaler.fit_transform(X_train_no_text_encoded2)
X_valid_no_text_encoded2 = scaler.transform(X_valid_no_text_encoded2)
X_test_no_text_encoded2 = scaler.transform(X_test_no_text_encoded2)

y_train = np.load('engineered_datasets/y_train.npy')
y_valid = np.load('engineered_datasets/y_valid.npy')
y_train1 = y_train.tolist()
y_valid1 = y_valid.tolist()


# X_train_no_text_encoded2.shape
num_features = X_train_no_text_encoded2.shape[-1]


# %% tokenisation and target label alignment with the huggingface library

"""
Before feeding the texts to a Transformer model, we will need to tokenize our input using a huggingface Transformer tokenizer. 
!!! It is crucial that the tokenizer we use must match the Transformer model type you are using
we will use the huggingface BertTokenizerFast tokenizer, which standardizes the length of our sequence to 512 and pads with zeros. Notice this matches the maximum length we used when creating tags.
"""

"""
Transformer models are often trained by tokenizers that split words into subwords. 
For instance, the word 'Africa' might get split into multiple subtokens. 
This can create some misalignment between [the list of target labels from the labeled dataset] and [the list of labels generated by the tokenizer], since the tokenizer can split one word into several, or add special tokens. 
Before processing, it is important that we align the lists of target labels and the list of labels generated by the selected tokenizer.

Our problem is many-to-one; the target of each instance is one label, so we can forgo the trouble of aligning labels of sub-words.
All we need to do is to tokenise and pad sentences
"""

tokenizer = BertTokenizerFast.from_pretrained(
    'pre-trained-transformer-bert-base-uncased/')


max_len = 512

X_train_tokenized = tokenizer(X_train_text_preprocessed, truncation=True,
                              is_split_into_words=False, padding='max_length', max_length=max_len)
X_train_tokenized_ids = X_train_tokenized['input_ids']


X_valid_tokenized = tokenizer(X_valid_text_preprocessed, truncation=True,
                              is_split_into_words=False, padding='max_length', max_length=max_len)
X_valid_tokenized_ids = X_valid_tokenized['input_ids']


X_test_tokenized = tokenizer(X_test_text_preprocessed, truncation=True,
                             is_split_into_words=False, padding='max_length', max_length=max_len)
X_test_tokenized_ids = X_test_tokenized['input_ids']


"""
examples
"""

# tokenizer.tokenize('hello there')
# # ['hello', 'there']
# tokenizer('hello there', truncation=True, is_split_into_words=False, padding='max_length', max_length=10)
# # {'input_ids': [101, 7592, 2045, 102, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}
# # each subword has an 'input_id': 101 is start of string and 102 is end of string
# tokenizer('hello there', truncation=True, is_split_into_words=False, padding='max_length', max_length=10).word_ids()
# # [None, 0, 1, None, None, None, None, None, None, None]
# # word_ids assigns, in ascending order, the same word id for all sub-words of the same original word

# example1 = labeledTrainData['review_preprocessed'].iloc[:1].values.tolist()[0]
# np.array(tokenizer.tokenize(example1))
# # array(['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the',
# #        'moment', 'with', 'm', '##j', 'i', 've', 'started', 'listening',
# #        'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary',
# #        'here', 'and', 'there', 'watched', 'the', 'wi', '##z', 'and',
# #        'watched', 'moon', '##walker', ..., 'most',
# #        'sick', '##est', 'liar', '##s', 'i', 'hope', 'he', 'is', 'not',
# #        'the', 'latter'], dtype='<U11')
# # so it splits words into subwords

# tokenizer(example1, truncation=True, is_split_into_words=False, padding='max_length', max_length=20)
# # {'input_ids': [101, 2007, 2035, 2023, 4933, 2183, 2091, 2012, 1996, 2617, 2007, 1049, 3501, 1045, 2310, 2318, 5962, 2000, 2010, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
# # each subword has an 'input_id': 101 is start of string and 102 is end of string
# tokenizer(example1, truncation=True, is_split_into_words=False, padding='max_length', max_length=20).word_ids()
# # [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 11, 12, 13, 14, 15, 16, None]
# # word_ids assigns, in ascending order, the same word id for all sub-words of the same original word

# tokenizer(labeledTrainData['review_preprocessed'].iloc[0:4].values.tolist(), truncation=True, is_split_into_words=False, padding='max_length', max_length=20)

# # {'input_ids': [[101, 2007, 2035, 2023, 4933, 2183, 2091, 2012, 1996, 2617, 2007, 1049, 3501, 1045, 2310, 2318, 5962, 2000, 2010, 102], [101, 1996, 4438, 2162, 1997, 1996, 8484, 2011, 10805, 25445, 2003, 1037, 2200, 14036, 2143, 2008, 5525, 3632, 2000, 102], [101, 1996, 2143, 4627, 2007, 1037, 3208, 6141, 4330, 3228, 6160, 9387, 2728, 12385, 17190, 2638, 2000, 22289, 2380, 102], [101, 2009, 2442, 2022, 5071, 2008, 2216, 2040, 5868, 2023, 2143, 1996, 4602, 6361, 3850, 2412, 2134, 1056, 1045, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}

""" ---------------- end of example ----------------"""


# %% optimisation

"""
feed data into a pretrained ðŸ¤— model. optimize a DistilBERT model, which matches the tokenizer used to preprocess your data
"""


Bert_trans_model = TFBertForSequenceClassification.from_pretrained(
    'pre-trained-transformer-bert-base-uncased/', num_labels=1)

# load pre-trained(x2) weights
# Bert_trans_model.load_weights('transformer_model_weights.h5')

# example2 = X_train_tokenized['input_ids'][0]

# label inputs are in a list [label1, label2, ...]
# Bert_trans_model.fit([example2, example2], [1,1])

# Bert_trans_model.predict([example2])
# # TFSequenceClassifierOutput(loss=None, logits=array([[0.30842814]], dtype=float32), hidden_states=None, attentions=None)

# import tensorflow_addons as tfa
# tfa.metrics.F1Score(num_classes=1, threshold = 0.5)

my_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)
Bert_trans_model.compile(loss=my_loss, optimizer=optimizer, metrics=[
                         'accuracy', tf.keras.metrics.AUC(from_logits=True)])

# callback = tf.keras.callbacks.EarlyStopping(
#     monitor='val_loss', patience=2, restore_best_weights=True)

# if you get GPU 'Resource exhausted: failed to allocate memory', reduce batch_size
history = Bert_trans_model.fit(X_train_tokenized_ids, y_train1, validation_data=(
    X_valid_tokenized_ids, y_valid1), epochs=2, batch_size=4)

# Epoch 1/4
# 1523/1523 [==============================] - 535s 352ms/step - loss: 0.4728 - accuracy: 0.7714 - auc: 0.8444 - val_loss: 0.4060 - val_accuracy: 0.8313 - val_auc: 0.8917
# Epoch 2/4
# 1523/1523 [==============================] - 531s 349ms/step - loss: 0.3464 - accuracy: 0.8604 - auc: 0.9145 - val_loss: 0.3924 - val_accuracy: 0.8345 - val_auc: 0.8919
# Epoch 3/4
# 1523/1523 [==============================] - 524s 344ms/step - loss: 0.2629 - accuracy: 0.8980 - auc: 0.9482 - val_loss: 0.4083 - val_accuracy: 0.8286 - val_auc: 0.8879
# Epoch 4/4
# 1523/1523 [==============================] - 514s 337ms/step - loss: 0.1863 - accuracy: 0.9342 - auc: 0.9717 - val_loss: 0.4586 - val_accuracy: 0.8372 - val_auc: 0.8821


"""# save the model"""

# Bert_trans_model.save_weights('transformer_model_weights.h5')


# %% predict

def sigmoid(x):
    return 1 / (1 + np.exp(-x))


prediction = Bert_trans_model.predict(X_test_tokenized_ids).logits    # logits
prediction = sigmoid(prediction)  # apply sigmoid
prediction_bool = (prediction > 0.5).astype(int)
prediction_bool = prediction_bool.squeeze()
test_df = pd.read_csv("test.csv", header=[0])
transformer_pred = pd.Series(
    prediction_bool, index=test_df['id'], name='target')
transformer_pred.to_csv('transformer_pred.csv')


# 0.83021


# tf.keras.backend.clear_session()

# from numba import cuda
# device = cuda.get_current_device()
# device.reset()
